模型转换及量化常见问题
--------------------------

开发文档：
《 `TPU-MLIR快速入门手册 <https://doc.sophgo.com/sdk-docs/v23.09.01-lts/docs_latest_release/docs/tpu-mlir/quick_start/html/index.html>`_ 》，《 `TPU-MLIR开发参考手册 <https://doc.sophgo.com/sdk-docs/v23.09.01-lts/docs_latest_release/docs/tpu-mlir/developer_manual/html/index.html>`_ 》。

基本使用
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~


模型转换失败怎么办？ 
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

答： 

#. 检查命令行输入参数有没有错误，这个一般会有打印提示；
#. 不支持的算子需要开发，可以联系我方技术人员来解决；
#. 使用转换工具编译模型时，保存更详细的日志，提供给我方技术人员进一步排查；
#. 有时转换失败是因为误差比对超过了允许的阈值而导致编译过程中断，目前比对误差阈值设置为误差在0.01之内，但也不排除有些模型有很多的累加或除法操作，由于尾差累计导致超出这个范围；可以加上--cmp False关闭比对，最终到业务层面上验证转换后的模型精度是否符合要求；


是否支持模型的在线编译？
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

答：不支持，也不推荐这样做，因为模型编译的过程是特别耗时的。因此我们采用了离线编译生成BModel，在线推理时直接加载BModel运行的方式。

fp32模型转换
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

fp32模型的输出和原始模型输出差异比较大怎么办？
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

答： 

#. 对于两个模型采用同样的输入，看看输出是否一致，比如输入都是用全部是0.1的矩阵，填充input tensor的内存空间，然后做推理，比较输出数据的差异;
#. 设置 export BMRT_SAVE_IO_TENSORS=1，运行bmruntime的时候会输出两个文件 input_ref_data.dat.bmrt和output_ref_data.dat.bmrt，用这个和理想的输入输出作对比排查。


int8模型量化
~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~

int8的输出和fp32模型输出差异比较大怎么办？
^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^

答： 

#. 检查前后处理是否有问题，int8网络输入输出一般需要做scale处理，看看是否遗漏？
#. 通过MLIR工具找出导致误差较大的层，参考 《 `量化与量化调优 <https://doc.sophgo.com/sdk-docs/v23.09.01-lts/docs_latest_release/docs/tpu-mlir/quick_start/html/07_quantization.html>`_ 》。


